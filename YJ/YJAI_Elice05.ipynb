{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8748913987836664\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "# 데이터를 불러옵니다.\n",
    "datafile = open('spambase.data', 'r')\n",
    "data = []\n",
    "for line in datafile:\n",
    "    line = [float(element) for element in line.rstrip('\\n').split(',')]\n",
    "    data.append(np.asarray(line))\n",
    "\n",
    "num_features = 48\n",
    "\n",
    "X = [data[i][:num_features] for i in range(len(data))]\n",
    "y = [int(data[i][-1]) for i in range(len(data))]\n",
    "\n",
    "# train, test 데이터셋으로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=21)\n",
    "class Naive_Bayes_Classifier(object):\n",
    "    # 48개의 feature를 이용합니다\n",
    "    def __init__(self, num_features=48):\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def log_likelihoods_naivebayes(self, feature_vector, Class):\n",
    "        assert len(feature_vector) == self.num_features\n",
    "        log_likelihood = 0.0 #log-likelihood를 사용해 underflow 회피\n",
    "        if Class == 0:\n",
    "            for feature_index in range(len(feature_vector)):\n",
    "                if feature_vector[feature_index] == 1: #feature present\n",
    "                    log_likelihood += np.log10(self.likelihoods_ham[feature_index])\n",
    "                elif feature_vector[feature_index] == 0: #feature absent\n",
    "                    log_likelihood += np.log10(1.0 - self.likelihoods_ham[feature_index])\n",
    "        elif Class == 1:\n",
    "            for feature_index in range(len(feature_vector)):\n",
    "                if feature_vector[feature_index] == 1:\n",
    "                    log_likelihood += np.log10(self.likelihoods_spam[feature_index])\n",
    "                elif feature_vector[feature_index] == 0:\n",
    "                    log_likelihood += np.log10(1.0 - self.likelihoods_spam[feature_index])\n",
    "        else:\n",
    "            raise ValueError(\"Class takes integer values 0 or 1\")\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    # Maximum A Priori(MAP) inference를 이용해 사후확률이 가장 큰 class를 고르기\n",
    "    def class_posteriors(self, feature_vector):\n",
    "        log_likelihood_ham = self.log_likelihoods_naivebayes(feature_vector, Class = 0)\n",
    "        log_likelihood_spam = self.log_likelihoods_naivebayes(feature_vector, Class = 1)\n",
    "\n",
    "        log_posterior_ham = log_likelihood_ham + self.log_prior_ham\n",
    "        log_posterior_spam = log_likelihood_spam + self.log_prior_spam\n",
    "\n",
    "        return log_posterior_ham, log_posterior_spam\n",
    "\n",
    "    def spam_classify(self, document):\n",
    "        feature_vector = [int(element>0.0) for element in document]\n",
    "        log_posterior_ham, log_posterior_spam = self.class_posteriors(feature_vector)\n",
    "        if log_posterior_ham > log_posterior_spam:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "        # 모델을 학습하는 train 함수를 작성하세요.\n",
    "    def train(self, X_train, y_train):\n",
    "# Likelihood estimator 만들기\n",
    "# 스팸 클래스와 햄 클래스 나누기\n",
    "        X_train_spam = [X_train[i] for i in range(len(X_train)) if y_train[i] == 1]\n",
    "        X_train_ham = [X_train[i] for i in range(len(X_train)) if y_train[i] == 0]\n",
    "\n",
    "        # 각 클래스의 feature에 대한 likelihood 구하기\n",
    "        self.likelihoods_ham = np.mean(X_train_ham, axis = 0)/100.0\n",
    "        self.likelihoods_spam = np.mean(X_train_spam, axis = 0)/100.0\n",
    "\n",
    "        # 각 class의 prior를 계산\n",
    "        num_ham = float(len(X_train_ham))\n",
    "        num_spam = float(len(X_train_spam))\n",
    "\n",
    "        prior_probability_ham = num_ham / (num_ham + num_spam)\n",
    "        prior_probability_spam = num_spam / (num_ham + num_spam)\n",
    "\n",
    "        self.log_prior_ham = np.log10(prior_probability_ham)\n",
    "        self.log_prior_spam = np.log10(prior_probability_spam)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for case in X_test:\n",
    "            predictions.append(self.spam_classify(case))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "NB = Naive_Bayes_Classifier()\n",
    "NB.train(X_train, y_train)\n",
    "pred = NB.predict(X_test)\n",
    "\n",
    "def evaluate_performance(predictions, ground_truth_labels):\n",
    "    correct_count = 0.0\n",
    "    for item_index in range(len(predictions)):\n",
    "        if predictions[item_index] == ground_truth_labels[item_index]:\n",
    "            correct_count += 1.0\n",
    "    accuracy = correct_count/len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "accuracy_naivebayes = evaluate_performance(pred, y_test)\n",
    "print(accuracy_naivebayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.84      0.88        50\n",
      "          1       0.92      0.97      0.94        93\n",
      "\n",
      "avg / total       0.92      0.92      0.92       143\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.88      0.92        50\n",
      "          1       0.94      0.98      0.96        93\n",
      "\n",
      "avg / total       0.94      0.94      0.94       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "# 유방암 데이터 로드\n",
    "dataset = datasets.load_breast_cancer()\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size = 0.25, random_state=21)\n",
    "\n",
    "# (1) Gaussian Naive Bayes 모델을 만들고 데이터에 fit 하세요.\n",
    "model_gaussianNB = GaussianNB()\n",
    "model_gaussianNB.fit(X_train, y_train)\n",
    "print(model_gaussianNB)\n",
    "\n",
    "# (2) Logistic Regression 모델을 만들고 데이터에 fit 하세요.\n",
    "model_logisticReg = LogisticRegression()\n",
    "model_logisticReg.fit(X_train, y_train)\n",
    "print(model_logisticReg)\n",
    "\n",
    "# (3) Fitting 된 모델을 이용해 test 데이터의 label을 predict 하세요.\n",
    "expected = y_test\n",
    "pred_gaussianNB = model_gaussianNB.predict(X_test)\n",
    "pred_logisticReg = model_logisticReg.predict(X_test)\n",
    "\n",
    "# Prediction 확인\n",
    "print(metrics.classification_report(expected, pred_gaussianNB))\n",
    "print(metrics.classification_report(expected, pred_logisticReg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
